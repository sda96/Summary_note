{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습관련기술들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 매개변수 갱신에 대한 내용으로 올바르지 않은 것을 고르시오.\n",
    "매개변수 갱신\n",
    "1. 매개변수란 컴퓨터가 수정이 가능한 변수입니다. \n",
    "2. 매개변수의 기울기는 갱신되어질때 마다 변화하게 됩니다.\n",
    "3. 매개변수의 기울기는 신경망 학습 목적에 맞게 손실 함수가 가장 낮아지는 방향으로 갱신되어집니다.\n",
    "4. 갱신되어진 기울기 방향으로 갱신을 반복하여 최적의 값으로 다가가는 방법을 확률적 경사 하강법이라고 부릅니다.\n",
    "5. 신경망의 유닛이 증가할 수록 매개변수의 수도 증가하게됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 확률적 경사하강법에 대한 내용으로 옳지 않은 것을 고르시오,\n",
    "확률적 경사하강법\n",
    "1. 확률적 경사하강법은(Stochastic Gradient Descent) 손실함수의 값을 최소로하는 매개변수를 찾는 최적화 기법입니다.\n",
    "2. SGD는 기울기를 사용하는데 이 기울기는 가중치의 변화량에 따른 손실함수의 변화량입니다. \n",
    "3. SGD의 수식은 $\\hat{W} = W - \\frac{\\partial Loss}{\\partial W} * lr $로 $W$는 갱신되기전의 가중치이고 $\\hat{W}$는 갱신된 후의 가중치입니다.\n",
    "4. SGD는 순간 기울기의 반대 방향으로 일정거리(학습률)만큼 가중치를 갱신켜주는 방법입니다.\n",
    "5. SGD의 단점은 비등방성 함수에서는 탐색 경로가 비효율적이라는 것 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모멘텀에 대한 내용으로 올바르지 않은 것을 고르시오.\n",
    "모멘텀\n",
    "1. 모멘텀 기법은 경사하강법에 물리법칙은 적용하여 운동량이라는 모멘텀 개념을 추가한 방법입니다.\n",
    "2. 모멘텀의 수식은 $\\hat{v} = av - \\frac{\\partial Loss}{\\partial W} * lr$, 속도를 구한 뒤 $\\hat{W} = W + \\hat{v}$을 이용하여 가중치를 갱신시킵니다.\n",
    "3. v의 식은 기존의 경사하강법에 기울기 방향으로 힘을 받아 가속되는 관성이라는 물리법칙 개념을 추가한 식입니다.\n",
    "4. av항은 기울기가 서서히 변화되도록 만들어주는 역할을 하는 항입니다.\n",
    "5. 모멘텀 기법을 사용하면 기존의 SGD보다 최적화되는 값에 다가가는 속도가 빠릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad에 대한 내용으로 올바르지 않은 것을 고르시오.\n",
    "AdaGrad\n",
    "1. 신경망 학습에서 학습률에 따라서 작으면 학습시간이 길어지고 너무 크면 학습이 제대로 이뤄지지 않습니다.\n",
    "1. 각 매개변수에 맞는 학습률을 만들어주는 방법으로 AdaGrad가 있습니다.\n",
    "3. AdaGrad는 기존의 기울기값의 제곱을 활용하여, 매개변수가 갱신될 때 학습률도 같이 갱신되어집니다.\n",
    "4. 기울기값이 크게 변하는 매개변수의 학습률은 더 작아지도록 만들어 줍니다.\n",
    "5. 학습률의 감소는 매개변수마다 다르게 적용되어 집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam에 대한 내용으로 올바르지 않은 것을 고르시오.\n",
    "Adam\n",
    "1. 모멘텀 기법과 AdaGrad 방법을 적용한 기법을 Adam이라고 부릅니다.\n",
    "2. Adam은 매개변수 공간에서 효율적으로 최적의 매개변수를 찾아줍니다.\n",
    "3. 하이퍼파라미터로 '편향 보정'이 진행된다는 것도 Adam의 특징 입니다.\n",
    "4. Adam의 가중치 갱신과정은 모멘텀과도 비슷하고 학습률도 조정하기에 더 효율적으로 손실함수의 최솟값을 찾아갑니다.\n",
    "5. Adam은 3개의 하이퍼파라미터를 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 대부분의 최적화 방법으로는 Adam를 사용하는 것이 가장 좋습니다.\n",
    "최적의 최적화기법 선택  \n",
    "Yes / No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정답 : No 최적화 기법은 풀어야할 문제가 무엇이냐, 하이퍼파라미터를 어떻게 지정하느냐에 따라서 다르기 때문에 상황에 맞는 적절한 최적화 기법을 사용하는 것이 가장 좋습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가중치의 초깃값에 대한 내용으로 올바르지 않은 것을 고르시오.\n",
    "가중치 초깃값  \n",
    "1. 가중치 감소기법은 과대적합을 억제시켜 범용성을 높여주는 기술입니다.\n",
    "2. 가중치를 0으로 잡고 시작하면 가중치가 똑같은 값으로 갱신되기 때문에 모델은 올바른 학습이 이뤄지지 않게 됩니다.\n",
    "3. 은닉층의 활성화값 분포를 통하여 가중치의 초깃값에 따른 은닉층의 변화를 확인할 수 있습니다.\n",
    "4. 표준편차가 1인 정규분포를 가중치 초깃값으로 사용하면 시그모이드 함수를 사용하는 모델의 경우 기울기 소실이 발생합니다.\n",
    "5. 표준편차를 0.01인 정규분포를 가중치 초깃값으로 사용하면 활성화값들이 치우쳐지면서 표현력을 제한하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier 초깃값에 대한 내용으로 올바르지 않은 것을 고르시오.\n",
    "Xavier 초깃값\n",
    "1. 각 층의 활성화값이 고르게 분포되도록 Xavier 초깃값을 사용해야 합니다.\n",
    "2. Xavier 초깃값은 앞 층의 노드가 n개라면 표준편차가 $\\frac{1}{\\sqrt{n}}$인 정규분포인 가중치 초깃값을 가집니다.\n",
    "3. Xavier 초깃값을 사용하면 앞 층의 노드 개수가 많아질수록 대상 노드의 초깃값으로 설정하는 가중치가 좁게 퍼집니다.\n",
    "4. 층이 깊어져도 가중치가 살아남기에 가중치 소실 문제가 발생하지 않게 됩니다.\n",
    "5. 층이 깊어지면서 일그러지는 활성화값 분포는 활성화 함수로 tanh 함수를 사용하면 깔끔한 분포가 나오게 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 활성화 함수를 ReLu를 사용할 경우 가중치 초깃값으로 Xavier 초깃값을 사용하는 것이 좋습니다.\n",
    "He 초깃값  \n",
    "Yes / No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정답 : No 활성화 함수로 ReLu를 사용하는 경우 가중치 초깃값으로는 He 초깃값을 사용해야 더 적절한 활성화값 분포를 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 배치 정규화 알고리즘에 대한 내용으로 올바른 것을 고르시오.\n",
    "배치 정규화\n",
    "1. 배치 정규화의 이점으로는 빠른 학습 속도, 초깃값에 비의존성, 과대적합 억제등이 있습니다.\n",
    "2. 배치 정규화의 기본 아이디어는 각 층에서 활성화값이 적당히 퍼지도록 조정하는 것 입니다.\n",
    "3. 배치 정규화는 학습시 미니배치 단위로 표준정규분포로 정규화를 진행합니다.\n",
    "4. 배치 정규화 처리를 활성화 함수 앞 또는 뒤에 적용함으로써 데이터의 분포가 덜 치우치게 만들어줍니다.\n",
    "5. 배치 정규화 계층에서는 정규화된 데이터에 고유한 확대와 이동변환을 수행하며 수식으로는 $y_i \\leftarrow \\gamma \\hat{x_i} + \\beta$ 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 과대적합과 과소적합에 대한 내용으로 올바른 것을 고르시오.\n",
    "과대적합과 과소적합\n",
    "1. 신경망이 훈련 데이터에 과도하게 적합되어져 테스트 데이터에서 성능이 좋지 않은 상태를 과대적합이라고 부릅니다.\n",
    "2. 훈련 데이터와 테스트 데이터 모두에서 성능이 좋지 못한 상태를 과소적합이라고 부릅니다.\n",
    "3. 과대적합은 매개변수가 많은 복잡한 모델이나 훈련 데이터가 부족한 경우 발생하며 가중치 감소와 드롭아웃 방법으로 완화시킬 수 있습니다.\n",
    "4. 가중치 감소는 큰 가중치에 대해서는 그에 상응하는 큰 페널티를 부과하여 과대적합을 억제하는 방법입니다.\n",
    "5. 드롭아웃은 훈련때는 데이터를 흘릴 때마다 삭제할 뉴런을 무작위로 선택하고, 테스트 때는 모든 뉴런에 신호를 전달하는 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 적절한 하이퍼파라미터를 찾는 내용으로 올바르지 않은 것을 고르시오.\n",
    "하이퍼파라미터 튜닝\n",
    "1. 적절한 하이퍼파라미터 값은 모델의 성능에 큰 영향을 주지만 일반적으로 많은 시행착오를 겪습니다.\n",
    "2. 하이퍼파라미터를 찾아가는 과정은 처음엔 대략적인 범위를 설정하고 그 범위를 최적 값에 가까워지도록 좁혀줍니다.\n",
    "3. 하이퍼파라미터 최적화에서는 그리드 서치와 같은 탐색보다 무작위로 샘플링해 탐색하는 편이 좋다고 알려져 있습니다.\n",
    "4. 훈련 데이터로 매개변수를 갱신시키고 검증 데이터로 적절한 하이퍼파라미터 값을 찾습니다.\n",
    "5. 테스트 데이터로 완성되어진 모델의 범용 성능을 확인합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
